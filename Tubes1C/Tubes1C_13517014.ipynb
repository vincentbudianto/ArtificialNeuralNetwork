{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar 1C IF3270 - Machine Learning\n",
    "------------------------------------------\n",
    "##### NIM/Nama  : 13517014/Yoel Susanto | 13517065/Andrian Cedric | 13517131/Jan Meyer Saragih | 13517137/Vincent Budianto\n",
    "##### Nama file : Tubes1C_13517014.ipynb\n",
    "##### Topik     : Implementasi modul myMLP\n",
    "##### Tanggal   : 28 February 2020\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Layer import Layer, ActivationFunction\n",
    "from MLP import MLP\n",
    "from sklearn.neural_network import MLPClassifier as skMLP\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets as dataset\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.tree as tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Penjelasan Implementasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementasi myMLP\n",
    "\n",
    "20:05 Jan Meyer Implementasi myMLP dari kelompok kami menggunakan proses yang digunakan pada MLP pada umumnya, yaitu melakukan feed forward feedforward untuk menentukan keputusan yang ada berdasarkan data training yang dilatih dan data test yang diuji dan memperbaiki hasilnya dengan backpropagation, yaitu algoritma neural network yang memperbaiki weight setiap node dalam layer.\n",
    "\n",
    "Pada implementasi MLP kami , logika komputasi didalam layer dibuat didalam Layer.py, terdapat class Layer yang mengandung informasi intrinsik sebuah Layer, misalnya jumlah node pada layer tersebut, weight dari layer sebelumnya, delta dan sebagainya. Selain itu terdapat juga class ActivationFunction yang merupakan enumerasi dari berbagai jenis fungsi aktivasi seperti fungsi linear dan sigmoid.\n",
    "\n",
    "Class Layer akan menerima inputan berupa jumlah node, lapisan layer ke-berapa, dan jumlah node pada layer sebelumnya. Kemudian, setiap input yang masuk akan dicek apakah itu adalah layer pertama atau bukan pertama. Jika bukan pertama, akan ditambahkan bias. Kemudian, setiap layer yang terbentuk akan diberi fungsi aktivasi sesuai jenis layernya. Bagi layer pertama, fungsi aktivasi yang digunakan adalah linear, hal ini disebabkan dalam implementasi MLP kami, input layer tidak melakukan komputasi melainkan hanya meneruskan input kepada hidden layer. Bagi layer kedua dan layer output, fungsi aktivasi yang digunakan adalah sigmoid\n",
    "\n",
    "Kemudian, setiap layer yang terbentuk akan diolah di dalam class MLP di file MLP.py dimana data-data akan selalu direset dengan flush dan diisi dengan delta sesuai dengan layernya melalui flushDelta. Kemudian, data yang telah masuk akan di-feed forward untuk nambahkan nilai dari node awal ke nilai akhir. Setelah itu, baru dijalankan algoritma backpropagation, yaitu mencari nilai delta atribut, delta hidden unit, dan delta weight untuk diterapkan pada setiap layer. Itu dilakukan untuk setiap data yang ada.\n",
    "\n",
    "Setiap data diperoleh dari dataset yang discan dan memiliki dikelompokkan ke dalam mini-batch yang setiapnya berisi 10 data. flushDelta akan dipanggil setiap 1 mini-batch selesai diproses untuk meng-update weight. Setelah semua mini-batch selesai diproses, maka 1 epoch selesai.\n",
    "\n",
    "Setelah itu, akan dijalankan predictionValue dan melakukan fungsi pembelajaran (learn) yang terhenti ketika mencapai max iteration, mencapai error minimum, atau mulai diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Hasil Eksekusi dan Perbandingan dengan hasil MLP sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv\n",
    "data = pd.read_csv(\"iris.csv\")\n",
    "predictData = data\n",
    "dataHead = list(data.columns)\n",
    "\n",
    "# Shuffle data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Creates data by 10 (indexes of data)\n",
    "dataSplitCount = 10\n",
    "dataDict = {n: data.iloc[n:n+dataSplitCount, :] for n in range(0, len(data), dataSplitCount)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. myMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model generator\n",
    "Generates layer to be put in MLP class and return MLP Model\n",
    "\n",
    "Current layer:\n",
    "    x\n",
    "        x   x\n",
    "    x\n",
    "GK      x   x\n",
    "    x\n",
    "        x   x\n",
    "    x\n",
    "\n",
    "Abaikan GK\n",
    "Layer paling kiri adalah keempat input dari 4 atribut iris.csv\n",
    "Layer tengah adalah hidden layer\n",
    "Layer output berfungsi sebagai yang dijelaskan di fungsi outputCheck\n",
    "'''\n",
    "def generateModel(learningRate):\n",
    "    layer0 = Layer(4, 0, 4, ActivationFunction.linear)\n",
    "    layer1 = Layer(3, 1, 4, ActivationFunction.sigmoid)\n",
    "    layer2 = Layer(3, 2, 3, ActivationFunction.sigmoid)\n",
    "    layers = []\n",
    "    layers.append(layer0)\n",
    "    layers.append(layer1)\n",
    "    layers.append(layer2)\n",
    "    return MLP(layers, learningRate)\n",
    "\n",
    "# Do data training\n",
    "'''\n",
    "Create a function that returns the tuple of output node\n",
    "'''\n",
    "def nodeOutputCheck(str):\n",
    "    if (str == \"Versicolor\"):\n",
    "        return [0, 0, 1]\n",
    "    elif (str == \"Virginica\"):\n",
    "        return [0, 1, 0]\n",
    "    elif (str == \"Setosa\"):\n",
    "        return [1, 0, 0]\n",
    "    else:\n",
    "        return [1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ": 71, Wrong Prediction: 7, Total Case: 150, Error: 3.95062, Accuracy: 95.33%\nIteration: 72, Wrong Prediction: 7, Total Case: 150, Error: 3.91842, Accuracy: 95.33%\nIteration: 73, Wrong Prediction: 7, Total Case: 150, Error: 3.8838, Accuracy: 95.33%\nIteration: 74, Wrong Prediction: 7, Total Case: 150, Error: 3.84562, Accuracy: 95.33%\nIteration: 75, Wrong Prediction: 7, Total Case: 150, Error: 3.8039, Accuracy: 95.33%\nIteration: 76, Wrong Prediction: 7, Total Case: 150, Error: 3.76036, Accuracy: 95.33%\nIteration: 77, Wrong Prediction: 7, Total Case: 150, Error: 3.7183, Accuracy: 95.33%\nIteration: 78, Wrong Prediction: 6, Total Case: 150, Error: 3.68214, Accuracy: 96.0%\nIteration: 79, Wrong Prediction: 6, Total Case: 150, Error: 3.65619, Accuracy: 96.0%\nIteration: 80, Wrong Prediction: 6, Total Case: 150, Error: 3.64156, Accuracy: 96.0%\nIteration: 81, Wrong Prediction: 6, Total Case: 150, Error: 3.63419, Accuracy: 96.0%\nIteration: 82, Wrong Prediction: 6, Total Case: 150, Error: 3.6295, Accuracy: 96.0%\nIteration: 83, Wrong Prediction: 6, Total Case: 150, Error: 3.62584, Accuracy: 96.0%\nIteration: 84, Wrong Prediction: 6, Total Case: 150, Error: 3.62294, Accuracy: 96.0%\nIteration: 85, Wrong Prediction: 6, Total Case: 150, Error: 3.62063, Accuracy: 96.0%\nIteration: 86, Wrong Prediction: 6, Total Case: 150, Error: 3.61872, Accuracy: 96.0%\nIteration: 87, Wrong Prediction: 6, Total Case: 150, Error: 3.61698, Accuracy: 96.0%\nIteration: 88, Wrong Prediction: 7, Total Case: 150, Error: 3.61513, Accuracy: 95.33%\nIteration: 89, Wrong Prediction: 7, Total Case: 150, Error: 3.61293, Accuracy: 95.33%\nIteration: 90, Wrong Prediction: 7, Total Case: 150, Error: 3.61017, Accuracy: 95.33%\nIteration: 91, Wrong Prediction: 7, Total Case: 150, Error: 3.60672, Accuracy: 95.33%\nIteration: 92, Wrong Prediction: 7, Total Case: 150, Error: 3.60248, Accuracy: 95.33%\nIteration: 93, Wrong Prediction: 7, Total Case: 150, Error: 3.59741, Accuracy: 95.33%\nIteration: 94, Wrong Prediction: 7, Total Case: 150, Error: 3.59149, Accuracy: 95.33%\nIteration: 95, Wrong Prediction: 7, Total Case: 150, Error: 3.58473, Accuracy: 95.33%\nIteration: 96, Wrong Prediction: 7, Total Case: 150, Error: 3.57715, Accuracy: 95.33%\nIteration: 97, Wrong Prediction: 7, Total Case: 150, Error: 3.56878, Accuracy: 95.33%\nIteration: 98, Wrong Prediction: 5, Total Case: 150, Error: 3.55968, Accuracy: 96.67%\nIteration: 99, Wrong Prediction: 5, Total Case: 150, Error: 3.54988, Accuracy: 96.67%\nIteration: 100, Wrong Prediction: 5, Total Case: 150, Error: 3.53946, Accuracy: 96.67%\nIteration: 101, Wrong Prediction: 5, Total Case: 150, Error: 3.52848, Accuracy: 96.67%\nIteration: 102, Wrong Prediction: 5, Total Case: 150, Error: 3.51701, Accuracy: 96.67%\nIteration: 103, Wrong Prediction: 5, Total Case: 150, Error: 3.50512, Accuracy: 96.67%\nIteration: 104, Wrong Prediction: 5, Total Case: 150, Error: 3.4929, Accuracy: 96.67%\nIteration: 105, Wrong Prediction: 5, Total Case: 150, Error: 3.48041, Accuracy: 96.67%\nIteration: 106, Wrong Prediction: 5, Total Case: 150, Error: 3.46774, Accuracy: 96.67%\nIteration: 107, Wrong Prediction: 5, Total Case: 150, Error: 3.45497, Accuracy: 96.67%\nIteration: 108, Wrong Prediction: 5, Total Case: 150, Error: 3.44216, Accuracy: 96.67%\nIteration: 109, Wrong Prediction: 5, Total Case: 150, Error: 3.42939, Accuracy: 96.67%\nIteration: 110, Wrong Prediction: 5, Total Case: 150, Error: 3.41671, Accuracy: 96.67%\nIteration: 111, Wrong Prediction: 5, Total Case: 150, Error: 3.40416, Accuracy: 96.67%\nIteration: 112, Wrong Prediction: 5, Total Case: 150, Error: 3.39179, Accuracy: 96.67%\nIteration: 113, Wrong Prediction: 5, Total Case: 150, Error: 3.37963, Accuracy: 96.67%\nIteration: 114, Wrong Prediction: 5, Total Case: 150, Error: 3.3677, Accuracy: 96.67%\nIteration: 115, Wrong Prediction: 5, Total Case: 150, Error: 3.356, Accuracy: 96.67%\nIteration: 116, Wrong Prediction: 5, Total Case: 150, Error: 3.34456, Accuracy: 96.67%\nIteration: 117, Wrong Prediction: 5, Total Case: 150, Error: 3.33337, Accuracy: 96.67%\nIteration: 118, Wrong Prediction: 5, Total Case: 150, Error: 3.32243, Accuracy: 96.67%\nIteration: 119, Wrong Prediction: 5, Total Case: 150, Error: 3.31173, Accuracy: 96.67%\nIteration: 120, Wrong Prediction: 5, Total Case: 150, Error: 3.30128, Accuracy: 96.67%\nIteration: 121, Wrong Prediction: 5, Total Case: 150, Error: 3.29107, Accuracy: 96.67%\nIteration: 122, Wrong Prediction: 5, Total Case: 150, Error: 3.2811, Accuracy: 96.67%\nIteration: 123, Wrong Prediction: 5, Total Case: 150, Error: 3.27135, Accuracy: 96.67%\nIteration: 124, Wrong Prediction: 5, Total Case: 150, Error: 3.26181, Accuracy: 96.67%\nIteration: 125, Wrong Prediction: 5, Total Case: 150, Error: 3.2525, Accuracy: 96.67%\nIteration: 126, Wrong Prediction: 5, Total Case: 150, Error: 3.24339, Accuracy: 96.67%\nIteration: 127, Wrong Prediction: 5, Total Case: 150, Error: 3.23448, Accuracy: 96.67%\nIteration: 128, Wrong Prediction: 5, Total Case: 150, Error: 3.22577, Accuracy: 96.67%\nIteration: 129, Wrong Prediction: 5, Total Case: 150, Error: 3.21725, Accuracy: 96.67%\nIteration: 130, Wrong Prediction: 5, Total Case: 150, Error: 3.20892, Accuracy: 96.67%\nIteration: 131, Wrong Prediction: 5, Total Case: 150, Error: 3.20077, Accuracy: 96.67%\nIteration: 132, Wrong Prediction: 5, Total Case: 150, Error: 3.1928, Accuracy: 96.67%\nIteration: 133, Wrong Prediction: 5, Total Case: 150, Error: 3.18501, Accuracy: 96.67%\nIteration: 134, Wrong Prediction: 5, Total Case: 150, Error: 3.17738, Accuracy: 96.67%\nIteration: 135, Wrong Prediction: 5, Total Case: 150, Error: 3.16992, Accuracy: 96.67%\nIteration: 136, Wrong Prediction: 5, Total Case: 150, Error: 3.16263, Accuracy: 96.67%\nIteration: 137, Wrong Prediction: 5, Total Case: 150, Error: 3.1555, Accuracy: 96.67%\nIteration: 138, Wrong Prediction: 5, Total Case: 150, Error: 3.14852, Accuracy: 96.67%\nIteration: 139, Wrong Prediction: 5, Total Case: 150, Error: 3.14169, Accuracy: 96.67%\nIteration: 140, Wrong Prediction: 5, Total Case: 150, Error: 3.13502, Accuracy: 96.67%\nIteration: 141, Wrong Prediction: 5, Total Case: 150, Error: 3.12849, Accuracy: 96.67%\nIteration: 142, Wrong Prediction: 5, Total Case: 150, Error: 3.1221, Accuracy: 96.67%\nIteration: 143, Wrong Prediction: 5, Total Case: 150, Error: 3.11585, Accuracy: 96.67%\nIteration: 144, Wrong Prediction: 5, Total Case: 150, Error: 3.10973, Accuracy: 96.67%\nIteration: 145, Wrong Prediction: 5, Total Case: 150, Error: 3.10374, Accuracy: 96.67%\nIteration: 146, Wrong Prediction: 5, Total Case: 150, Error: 3.09787, Accuracy: 96.67%\nIteration: 147, Wrong Prediction: 5, Total Case: 150, Error: 3.09211, Accuracy: 96.67%\nIteration: 148, Wrong Prediction: 5, Total Case: 150, Error: 3.08646, Accuracy: 96.67%\nIteration: 149, Wrong Prediction: 5, Total Case: 150, Error: 3.08091, Accuracy: 96.67%\nIteration: 150, Wrong Prediction: 5, Total Case: 150, Error: 3.07545, Accuracy: 96.67%\nIteration: 151, Wrong Prediction: 5, Total Case: 150, Error: 3.07008, Accuracy: 96.67%\nIteration: 152, Wrong Prediction: 5, Total Case: 150, Error: 3.06479, Accuracy: 96.67%\nIteration: 153, Wrong Prediction: 5, Total Case: 150, Error: 3.05956, Accuracy: 96.67%\nIteration: 154, Wrong Prediction: 5, Total Case: 150, Error: 3.0544, Accuracy: 96.67%\nIteration: 155, Wrong Prediction: 5, Total Case: 150, Error: 3.04929, Accuracy: 96.67%\nIteration: 156, Wrong Prediction: 5, Total Case: 150, Error: 3.04424, Accuracy: 96.67%\nIteration: 157, Wrong Prediction: 5, Total Case: 150, Error: 3.03923, Accuracy: 96.67%\nIteration: 158, Wrong Prediction: 5, Total Case: 150, Error: 3.03427, Accuracy: 96.67%\nIteration: 159, Wrong Prediction: 5, Total Case: 150, Error: 3.02937, Accuracy: 96.67%\nIteration: 160, Wrong Prediction: 5, Total Case: 150, Error: 3.02453, Accuracy: 96.67%\nIteration: 161, Wrong Prediction: 5, Total Case: 150, Error: 3.01977, Accuracy: 96.67%\nIteration: 162, Wrong Prediction: 5, Total Case: 150, Error: 3.01512, Accuracy: 96.67%\nIteration: 163, Wrong Prediction: 5, Total Case: 150, Error: 3.01062, Accuracy: 96.67%\nIteration: 164, Wrong Prediction: 5, Total Case: 150, Error: 3.0063, Accuracy: 96.67%\nIteration: 165, Wrong Prediction: 5, Total Case: 150, Error: 3.00223, Accuracy: 96.67%\nIteration: 166, Wrong Prediction: 5, Total Case: 150, Error: 2.99844, Accuracy: 96.67%\nIteration: 167, Wrong Prediction: 5, Total Case: 150, Error: 2.99493, Accuracy: 96.67%\nIteration: 168, Wrong Prediction: 5, Total Case: 150, Error: 2.9916, Accuracy: 96.67%\nIteration: 169, Wrong Prediction: 5, Total Case: 150, Error: 2.98827, Accuracy: 96.67%\nIteration: 170, Wrong Prediction: 5, Total Case: 150, Error: 2.98475, Accuracy: 96.67%\nIteration: 171, Wrong Prediction: 5, Total Case: 150, Error: 2.9809, Accuracy: 96.67%\nIteration: 172, Wrong Prediction: 5, Total Case: 150, Error: 2.97671, Accuracy: 96.67%\nIteration: 173, Wrong Prediction: 5, Total Case: 150, Error: 2.97225, Accuracy: 96.67%\nIteration: 174, Wrong Prediction: 5, Total Case: 150, Error: 2.96763, Accuracy: 96.67%\nIteration: 175, Wrong Prediction: 5, Total Case: 150, Error: 2.96293, Accuracy: 96.67%\nIteration: 176, Wrong Prediction: 5, Total Case: 150, Error: 2.95822, Accuracy: 96.67%\nIteration: 177, Wrong Prediction: 5, Total Case: 150, Error: 2.95355, Accuracy: 96.67%\nIteration: 178, Wrong Prediction: 5, Total Case: 150, Error: 2.94893, Accuracy: 96.67%\nIteration: 179, Wrong Prediction: 5, Total Case: 150, Error: 2.94441, Accuracy: 96.67%\nIteration: 180, Wrong Prediction: 5, Total Case: 150, Error: 2.93998, Accuracy: 96.67%\nIteration: 181, Wrong Prediction: 5, Total Case: 150, Error: 2.93569, Accuracy: 96.67%\nIteration: 182, Wrong Prediction: 5, Total Case: 150, Error: 2.93154, Accuracy: 96.67%\nIteration: 183, Wrong Prediction: 5, Total Case: 150, Error: 2.92758, Accuracy: 96.67%\nIteration: 184, Wrong Prediction: 5, Total Case: 150, Error: 2.92383, Accuracy: 96.67%\nIteration: 185, Wrong Prediction: 5, Total Case: 150, Error: 2.92034, Accuracy: 96.67%\nIteration: 186, Wrong Prediction: 5, Total Case: 150, Error: 2.91714, Accuracy: 96.67%\nIteration: 187, Wrong Prediction: 5, Total Case: 150, Error: 2.91425, Accuracy: 96.67%\nIteration: 188, Wrong Prediction: 5, Total Case: 150, Error: 2.91167, Accuracy: 96.67%\nIteration: 189, Wrong Prediction: 5, Total Case: 150, Error: 2.90935, Accuracy: 96.67%\nIteration: 190, Wrong Prediction: 5, Total Case: 150, Error: 2.90716, Accuracy: 96.67%\nIteration: 191, Wrong Prediction: 5, Total Case: 150, Error: 2.90494, Accuracy: 96.67%\nIteration: 192, Wrong Prediction: 5, Total Case: 150, Error: 2.90247, Accuracy: 96.67%\nIteration: 193, Wrong Prediction: 5, Total Case: 150, Error: 2.8996, Accuracy: 96.67%\nIteration: 194, Wrong Prediction: 5, Total Case: 150, Error: 2.8962, Accuracy: 96.67%\nIteration: 195, Wrong Prediction: 5, Total Case: 150, Error: 2.89228, Accuracy: 96.67%\nIteration: 196, Wrong Prediction: 5, Total Case: 150, Error: 2.8879, Accuracy: 96.67%\nIteration: 197, Wrong Prediction: 5, Total Case: 150, Error: 2.88315, Accuracy: 96.67%\nIteration: 198, Wrong Prediction: 5, Total Case: 150, Error: 2.87814, Accuracy: 96.67%\nIteration: 199, Wrong Prediction: 5, Total Case: 150, Error: 2.87293, Accuracy: 96.67%\nIteration: 200, Wrong Prediction: 5, Total Case: 150, Error: 2.86765, Accuracy: 96.67%\nIteration: 201, Wrong Prediction: 5, Total Case: 150, Error: 2.86255, Accuracy: 96.67%\nIteration: 202, Wrong Prediction: 5, Total Case: 150, Error: 2.8579, Accuracy: 96.67%\nIteration: 203, Wrong Prediction: 5, Total Case: 150, Error: 2.85376, Accuracy: 96.67%\nIteration: 204, Wrong Prediction: 5, Total Case: 150, Error: 2.85004, Accuracy: 96.67%\nIteration: 205, Wrong Prediction: 5, Total Case: 150, Error: 2.84661, Accuracy: 96.67%\nIteration: 206, Wrong Prediction: 5, Total Case: 150, Error: 2.84336, Accuracy: 96.67%\nIteration: 207, Wrong Prediction: 5, Total Case: 150, Error: 2.84021, Accuracy: 96.67%\nIteration: 208, Wrong Prediction: 5, Total Case: 150, Error: 2.83714, Accuracy: 96.67%\nIteration: 209, Wrong Prediction: 5, Total Case: 150, Error: 2.8341, Accuracy: 96.67%\nIteration: 210, Wrong Prediction: 5, Total Case: 150, Error: 2.83109, Accuracy: 96.67%\nIteration: 211, Wrong Prediction: 5, Total Case: 150, Error: 2.82809, Accuracy: 96.67%\nIteration: 212, Wrong Prediction: 5, Total Case: 150, Error: 2.82509, Accuracy: 96.67%\nIteration: 213, Wrong Prediction: 5, Total Case: 150, Error: 2.82209, Accuracy: 96.67%\nIteration: 214, Wrong Prediction: 5, Total Case: 150, Error: 2.81907, Accuracy: 96.67%\nIteration: 215, Wrong Prediction: 5, Total Case: 150, Error: 2.81604, Accuracy: 96.67%\nIteration: 216, Wrong Prediction: 5, Total Case: 150, Error: 2.81299, Accuracy: 96.67%\nIteration: 217, Wrong Prediction: 5, Total Case: 150, Error: 2.80992, Accuracy: 96.67%\nIteration: 218, Wrong Prediction: 5, Total Case: 150, Error: 2.80685, Accuracy: 96.67%\nIteration: 219, Wrong Prediction: 5, Total Case: 150, Error: 2.80376, Accuracy: 96.67%\nIteration: 220, Wrong Prediction: 5, Total Case: 150, Error: 2.80067, Accuracy: 96.67%\nIteration: 221, Wrong Prediction: 5, Total Case: 150, Error: 2.79757, Accuracy: 96.67%\nIteration: 222, Wrong Prediction: 5, Total Case: 150, Error: 2.79446, Accuracy: 96.67%\nIteration: 223, Wrong Prediction: 5, Total Case: 150, Error: 2.79136, Accuracy: 96.67%\nIteration: 224, Wrong Prediction: 5, Total Case: 150, Error: 2.78827, Accuracy: 96.67%\nIteration: 225, Wrong Prediction: 5, Total Case: 150, Error: 2.78518, Accuracy: 96.67%\nIteration: 226, Wrong Prediction: 5, Total Case: 150, Error: 2.7821, Accuracy: 96.67%\nIteration: 227, Wrong Prediction: 5, Total Case: 150, Error: 2.77904, Accuracy: 96.67%\nIteration: 228, Wrong Prediction: 5, Total Case: 150, Error: 2.77599, Accuracy: 96.67%\nIteration: 229, Wrong Prediction: 5, Total Case: 150, Error: 2.77295, Accuracy: 96.67%\nIteration: 230, Wrong Prediction: 5, Total Case: 150, Error: 2.76994, Accuracy: 96.67%\nIteration: 231, Wrong Prediction: 5, Total Case: 150, Error: 2.76696, Accuracy: 96.67%\nIteration: 232, Wrong Prediction: 5, Total Case: 150, Error: 2.76399, Accuracy: 96.67%\nIteration: 233, Wrong Prediction: 5, Total Case: 150, Error: 2.76106, Accuracy: 96.67%\nIteration: 234, Wrong Prediction: 5, Total Case: 150, Error: 2.75816, Accuracy: 96.67%\nIteration: 235, Wrong Prediction: 5, Total Case: 150, Error: 2.7553, Accuracy: 96.67%\nIteration: 236, Wrong Prediction: 5, Total Case: 150, Error: 2.75248, Accuracy: 96.67%\nIteration: 237, Wrong Prediction: 5, Total Case: 150, Error: 2.7497, Accuracy: 96.67%\nIteration: 238, Wrong Prediction: 5, Total Case: 150, Error: 2.74696, Accuracy: 96.67%\nIteration: 239, Wrong Prediction: 5, Total Case: 150, Error: 2.74428, Accuracy: 96.67%\nIteration: 240, Wrong Prediction: 5, Total Case: 150, Error: 2.74165, Accuracy: 96.67%\nIteration: 241, Wrong Prediction: 5, Total Case: 150, Error: 2.73908, Accuracy: 96.67%\nIteration: 242, Wrong Prediction: 5, Total Case: 150, Error: 2.73657, Accuracy: 96.67%\nIteration: 243, Wrong Prediction: 5, Total Case: 150, Error: 2.73414, Accuracy: 96.67%\nIteration: 244, Wrong Prediction: 5, Total Case: 150, Error: 2.73177, Accuracy: 96.67%\nIteration: 245, Wrong Prediction: 5, Total Case: 150, Error: 2.72948, Accuracy: 96.67%\nIteration: 246, Wrong Prediction: 5, Total Case: 150, Error: 2.72727, Accuracy: 96.67%\nIteration: 247, Wrong Prediction: 5, Total Case: 150, Error: 2.72515, Accuracy: 96.67%\nIteration: 248, Wrong Prediction: 5, Total Case: 150, Error: 2.72312, Accuracy: 96.67%\nIteration: 249, Wrong Prediction: 5, Total Case: 150, Error: 2.72119, Accuracy: 96.67%\nIteration: 250, Wrong Prediction: 5, Total Case: 150, Error: 2.71935, Accuracy: 96.67%\nIteration: 251, Wrong Prediction: 5, Total Case: 150, Error: 2.71762, Accuracy: 96.67%\nIteration: 252, Wrong Prediction: 5, Total Case: 150, Error: 2.71599, Accuracy: 96.67%\nIteration: 253, Wrong Prediction: 5, Total Case: 150, Error: 2.71447, Accuracy: 96.67%\nIteration: 254, Wrong Prediction: 5, Total Case: 150, Error: 2.71306, Accuracy: 96.67%\nIteration: 255, Wrong Prediction: 5, Total Case: 150, Error: 2.71175, Accuracy: 96.67%\nIteration: 256, Wrong Prediction: 5, Total Case: 150, Error: 2.71056, Accuracy: 96.67%\nIteration: 257, Wrong Prediction: 5, Total Case: 150, Error: 2.70946, Accuracy: 96.67%\nIteration: 258, Wrong Prediction: 5, Total Case: 150, Error: 2.70846, Accuracy: 96.67%\nIteration: 259, Wrong Prediction: 5, Total Case: 150, Error: 2.70754, Accuracy: 96.67%\nIteration: 260, Wrong Prediction: 5, Total Case: 150, Error: 2.70668, Accuracy: 96.67%\nIteration: 261, Wrong Prediction: 5, Total Case: 150, Error: 2.70589, Accuracy: 96.67%\nIteration: 262, Wrong Prediction: 6, Total Case: 150, Error: 2.70512, Accuracy: 96.0%\nIteration: 263, Wrong Prediction: 6, Total Case: 150, Error: 2.70435, Accuracy: 96.0%\nIteration: 264, Wrong Prediction: 6, Total Case: 150, Error: 2.70354, Accuracy: 96.0%\nIteration: 265, Wrong Prediction: 6, Total Case: 150, Error: 2.70267, Accuracy: 96.0%\nIteration: 266, Wrong Prediction: 6, Total Case: 150, Error: 2.70166, Accuracy: 96.0%\nIteration: 267, Wrong Prediction: 6, Total Case: 150, Error: 2.70048, Accuracy: 96.0%\nIteration: 268, Wrong Prediction: 6, Total Case: 150, Error: 2.69905, Accuracy: 96.0%\nIteration: 269, Wrong Prediction: 6, Total Case: 150, Error: 2.69729, Accuracy: 96.0%\nIteration: 270, Wrong Prediction: 6, Total Case: 150, Error: 2.69511, Accuracy: 96.0%\nIteration: 271, Wrong Prediction: 6, Total Case: 150, Error: 2.69243, Accuracy: 96.0%\nIteration: 272, Wrong Prediction: 6, Total Case: 150, Error: 2.68911, Accuracy: 96.0%\nIteration: 273, Wrong Prediction: 6, Total Case: 150, Error: 2.68504, Accuracy: 96.0%\nIteration: 274, Wrong Prediction: 6, Total Case: 150, Error: 2.68008, Accuracy: 96.0%\nIteration: 275, Wrong Prediction: 6, Total Case: 150, Error: 2.67408, Accuracy: 96.0%\nIteration: 276, Wrong Prediction: 6, Total Case: 150, Error: 2.66684, Accuracy: 96.0%\nIteration: 277, Wrong Prediction: 6, Total Case: 150, Error: 2.65819, Accuracy: 96.0%\nIteration: 278, Wrong Prediction: 5, Total Case: 150, Error: 2.6479, Accuracy: 96.67%\nIteration: 279, Wrong Prediction: 5, Total Case: 150, Error: 2.63571, Accuracy: 96.67%\nIteration: 280, Wrong Prediction: 4, Total Case: 150, Error: 2.62129, Accuracy: 97.33%\nIteration: 281, Wrong Prediction: 4, Total Case: 150, Error: 2.60418, Accuracy: 97.33%\nIteration: 282, Wrong Prediction: 4, Total Case: 150, Error: 2.58344, Accuracy: 97.33%\nIteration: 283, Wrong Prediction: 4, Total Case: 150, Error: 2.55667, Accuracy: 97.33%\nIteration: 284, Wrong Prediction: 4, Total Case: 150, Error: 2.51776, Accuracy: 97.33%\nIteration: 285, Wrong Prediction: 4, Total Case: 150, Error: 2.5588, Accuracy: 97.33%\nIteration: 286, Wrong Prediction: 4, Total Case: 150, Error: 2.5445, Accuracy: 97.33%\nIteration: 287, Wrong Prediction: 4, Total Case: 150, Error: 2.50846, Accuracy: 97.33%\nIteration: 288, Wrong Prediction: 4, Total Case: 150, Error: 2.56285, Accuracy: 97.33%\nIteration: 289, Wrong Prediction: 4, Total Case: 150, Error: 2.5624, Accuracy: 97.33%\nIteration: 290, Wrong Prediction: 4, Total Case: 150, Error: 2.56437, Accuracy: 97.33%\nIteration: 291, Wrong Prediction: 4, Total Case: 150, Error: 2.5099, Accuracy: 97.33%\nIteration: 292, Wrong Prediction: 4, Total Case: 150, Error: 2.63166, Accuracy: 97.33%\nIteration: 293, Wrong Prediction: 4, Total Case: 150, Error: 2.63812, Accuracy: 97.33%\nIteration: 294, Wrong Prediction: 4, Total Case: 150, Error: 2.64053, Accuracy: 97.33%\nIteration: 295, Wrong Prediction: 4, Total Case: 150, Error: 2.6424, Accuracy: 97.33%\nIteration: 296, Wrong Prediction: 4, Total Case: 150, Error: 2.644, Accuracy: 97.33%\nIteration: 297, Wrong Prediction: 4, Total Case: 150, Error: 2.64548, Accuracy: 97.33%\nIteration: 298, Wrong Prediction: 4, Total Case: 150, Error: 2.64687, Accuracy: 97.33%\nIteration: 299, Wrong Prediction: 4, Total Case: 150, Error: 2.64819, Accuracy: 97.33%\nIteration: 300, Wrong Prediction: 4, Total Case: 150, Error: 2.64945, Accuracy: 97.33%\nIteration: 301, Wrong Prediction: 4, Total Case: 150, Error: 2.65065, Accuracy: 97.33%\n"
    }
   ],
   "source": [
    "model = generateModel(0.1)\n",
    "model.learn(dataDict, dataSplitCount, nodeOutputCheck, maxIteration=1000, minError=1, divergingMaxCount=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[1, 0, 0] [1, 0, 0] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 1, 0] False\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 1, 0] False\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 1, 0] False\nTest Prediction:\n[0, 0, 1] [0, 1, 0] False\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 0, 1] [0, 0, 1] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\nTest Prediction:\n[0, 1, 0] [0, 1, 0] True\n"
    }
   ],
   "source": [
    "model.predict(predictData, nodeOutputCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Layer: 0\n[[1]\n [1]\n [1]\n [1]]\nLayer: 1\n[[ 0.32148913  0.45764157  2.3153078  -3.28581657 -1.4545532 ]\n [-5.2479528  -4.92510752 -3.45121705  7.47247823  6.24807592]\n [ 1.95831773  0.77724573  1.46830511 -1.82105995 -1.81374146]]\nLayer: 2\n[[-3.67763966  6.58721345 -2.65448921  0.64507866]\n [-2.47828443 -2.17045227  5.52574239 -2.60876634]\n [ 2.86157331 -7.38211112 -5.57595907  1.05785671]]\n"
    }
   ],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    print(\"Layer: {}\".format(i))\n",
    "    print(np.matrix(model.layers[i].weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = data[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "target = data[['variety']].replace(['Setosa','Versicolor','Virginica'],[0,1,2])\n",
    "df = pd.concat([df_norm, target], axis=1)\n",
    "testDf = df\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "trainX = df[['sepal.length', 'sepal.width', 'petal.length', 'petal.width']]\n",
    "trainY = df[['variety']]\n",
    "\n",
    "testX = df_norm\n",
    "testY = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tion 17, loss = 1.09841908\nIteration 18, loss = 1.09773342\nIteration 19, loss = 1.09718462\nIteration 20, loss = 1.09680759\nIteration 21, loss = 1.09659550\nIteration 22, loss = 1.09651289\nIteration 23, loss = 1.09651077\nIteration 24, loss = 1.09653993\nIteration 25, loss = 1.09656033\nIteration 26, loss = 1.09654608\nIteration 27, loss = 1.09648620\nIteration 28, loss = 1.09638238\nIteration 29, loss = 1.09624497\nIteration 30, loss = 1.09608845\nIteration 31, loss = 1.09592734\nIteration 32, loss = 1.09577325\nIteration 33, loss = 1.09563329\nIteration 34, loss = 1.09550983\nIteration 35, loss = 1.09540116\nIteration 36, loss = 1.09530295\nIteration 37, loss = 1.09520970\nIteration 38, loss = 1.09511612\nIteration 39, loss = 1.09501810\nIteration 40, loss = 1.09491316\nIteration 41, loss = 1.09480043\nIteration 42, loss = 1.09468040\nIteration 43, loss = 1.09455434\nIteration 44, loss = 1.09442385\nIteration 45, loss = 1.09429036\nIteration 46, loss = 1.09415490\nIteration 47, loss = 1.09401794\nIteration 48, loss = 1.09387943\nIteration 49, loss = 1.09373891\nIteration 50, loss = 1.09359569\nIteration 51, loss = 1.09344901\nIteration 52, loss = 1.09329819\nIteration 53, loss = 1.09314268\nIteration 54, loss = 1.09298211\nIteration 55, loss = 1.09281627\nIteration 56, loss = 1.09264509\nIteration 57, loss = 1.09246854\nIteration 58, loss = 1.09228659\nIteration 59, loss = 1.09209919\nIteration 60, loss = 1.09190623\nIteration 61, loss = 1.09170751\nIteration 62, loss = 1.09150278\nIteration 63, loss = 1.09129174\nIteration 64, loss = 1.09107406\nIteration 65, loss = 1.09084941\nIteration 66, loss = 1.09061742\nIteration 67, loss = 1.09037777\nIteration 68, loss = 1.09013012\nIteration 69, loss = 1.08987414\nIteration 70, loss = 1.08960948\nIteration 71, loss = 1.08933581\nIteration 72, loss = 1.08905274\nIteration 73, loss = 1.08875991\nIteration 74, loss = 1.08845687\nIteration 75, loss = 1.08814320\nIteration 76, loss = 1.08781842\nIteration 77, loss = 1.08748203\nIteration 78, loss = 1.08713351\nIteration 79, loss = 1.08677231\nIteration 80, loss = 1.08639784\nIteration 81, loss = 1.08600951\nIteration 82, loss = 1.08560668\nIteration 83, loss = 1.08518870\nIteration 84, loss = 1.08475487\nIteration 85, loss = 1.08430445\nIteration 86, loss = 1.08383669\nIteration 87, loss = 1.08335076\nIteration 88, loss = 1.08284582\nIteration 89, loss = 1.08232096\nIteration 90, loss = 1.08177525\nIteration 91, loss = 1.08120768\nIteration 92, loss = 1.08061721\nIteration 93, loss = 1.08000274\nIteration 94, loss = 1.07936308\nIteration 95, loss = 1.07869704\nIteration 96, loss = 1.07800330\nIteration 97, loss = 1.07728052\nIteration 98, loss = 1.07652727\nIteration 99, loss = 1.07574202\nIteration 100, loss = 1.07492320\nIteration 101, loss = 1.07406913\nIteration 102, loss = 1.07317805\nIteration 103, loss = 1.07224809\nIteration 104, loss = 1.07127731\nIteration 105, loss = 1.07026364\nIteration 106, loss = 1.06920492\nIteration 107, loss = 1.06809885\nIteration 108, loss = 1.06694305\nIteration 109, loss = 1.06573499\nIteration 110, loss = 1.06447201\nIteration 111, loss = 1.06315133\nIteration 112, loss = 1.06177001\nIteration 113, loss = 1.06032498\nIteration 114, loss = 1.05881301\nIteration 115, loss = 1.05723072\nIteration 116, loss = 1.05557457\nIteration 117, loss = 1.05384084\nIteration 118, loss = 1.05202567\nIteration 119, loss = 1.05012499\nIteration 120, loss = 1.04813458\nIteration 121, loss = 1.04605002\nIteration 122, loss = 1.04386674\nIteration 123, loss = 1.04157996\nIteration 124, loss = 1.03918473\nIteration 125, loss = 1.03667593\nIteration 126, loss = 1.03404824\nIteration 127, loss = 1.03129620\nIteration 128, loss = 1.02841418\nIteration 129, loss = 1.02539638\nIteration 130, loss = 1.02223688\nIteration 131, loss = 1.01892963\nIteration 132, loss = 1.01546846\nIteration 133, loss = 1.01184713\nIteration 134, loss = 1.00805934\nIteration 135, loss = 1.00409874\nIteration 136, loss = 0.99995901\nIteration 137, loss = 0.99563387\nIteration 138, loss = 0.99111710\nIteration 139, loss = 0.98640266\nIteration 140, loss = 0.98148467\nIteration 141, loss = 0.97635754\nIteration 142, loss = 0.97101599\nIteration 143, loss = 0.96545515\nIteration 144, loss = 0.95967065\nIteration 145, loss = 0.95365869\nIteration 146, loss = 0.94741615\nIteration 147, loss = 0.94094070\nIteration 148, loss = 0.93423087\nIteration 149, loss = 0.92728621\nIteration 150, loss = 0.92010736\nIteration 151, loss = 0.91269619\nIteration 152, loss = 0.90505589\nIteration 153, loss = 0.89719105\nIteration 154, loss = 0.88910781\nIteration 155, loss = 0.88081387\nIteration 156, loss = 0.87231861\nIteration 157, loss = 0.86363309\nIteration 158, loss = 0.85477008\nIteration 159, loss = 0.84574403\nIteration 160, loss = 0.83657109\nIteration 161, loss = 0.82726893\nIteration 162, loss = 0.81785674\nIteration 163, loss = 0.80835502\nIteration 164, loss = 0.79878540\nIteration 165, loss = 0.78917049\nIteration 166, loss = 0.77953358\nIteration 167, loss = 0.76989844\nIteration 168, loss = 0.76028898\nIteration 169, loss = 0.75072906\nIteration 170, loss = 0.74124212\nIteration 171, loss = 0.73185093\nIteration 172, loss = 0.72257737\nIteration 173, loss = 0.71344209\nIteration 174, loss = 0.70446439\nIteration 175, loss = 0.69566197\nIteration 176, loss = 0.68705078\nIteration 177, loss = 0.67864492\nIteration 178, loss = 0.67045658\nIteration 179, loss = 0.66249595\nIteration 180, loss = 0.65477125\nIteration 181, loss = 0.64728877\nIteration 182, loss = 0.64005291\nIteration 183, loss = 0.63306625\nIteration 184, loss = 0.62632973\nIteration 185, loss = 0.61984267\nIteration 186, loss = 0.61360300\nIteration 187, loss = 0.60760736\nIteration 188, loss = 0.60185125\nIteration 189, loss = 0.59632920\nIteration 190, loss = 0.59103490\nIteration 191, loss = 0.58596132\nIteration 192, loss = 0.58110090\nIteration 193, loss = 0.57644561\nIteration 194, loss = 0.57198712\nIteration 195, loss = 0.56771684\nIteration 196, loss = 0.56362609\nIteration 197, loss = 0.55970611\nIteration 198, loss = 0.55594817\nIteration 199, loss = 0.55234361\nIteration 200, loss = 0.54888391\nIteration 201, loss = 0.54556072\nIteration 202, loss = 0.54236592\nIteration 203, loss = 0.53929158\nIteration 204, loss = 0.53633007\nIteration 205, loss = 0.53347402\nIteration 206, loss = 0.53071633\nIteration 207, loss = 0.52805022\nIteration 208, loss = 0.52546919\nIteration 209, loss = 0.52296704\nIteration 210, loss = 0.52053785\nIteration 211, loss = 0.51817601\nIteration 212, loss = 0.51587620\nIteration 213, loss = 0.51363336\nIteration 214, loss = 0.51144272\nIteration 215, loss = 0.50929977\nIteration 216, loss = 0.50720025\nIteration 217, loss = 0.50514017\nIteration 218, loss = 0.50311576\nIteration 219, loss = 0.50112350\nIteration 220, loss = 0.49916007\nIteration 221, loss = 0.49722240\nIteration 222, loss = 0.49530759\nIteration 223, loss = 0.49341297\nIteration 224, loss = 0.49153605\nIteration 225, loss = 0.48967454\nIteration 226, loss = 0.48782630\nIteration 227, loss = 0.48598939\nIteration 228, loss = 0.48416202\nIteration 229, loss = 0.48234255\nIteration 230, loss = 0.48052952\nIteration 231, loss = 0.47872159\nIteration 232, loss = 0.47691757\nIteration 233, loss = 0.47511640\nIteration 234, loss = 0.47331715\nIteration 235, loss = 0.47151900\nIteration 236, loss = 0.46972127\nIteration 237, loss = 0.46792336\nIteration 238, loss = 0.46612479\nIteration 239, loss = 0.46432517\nIteration 240, loss = 0.46252423\nIteration 241, loss = 0.46072173\nIteration 242, loss = 0.45891757\nIteration 243, loss = 0.45711168\nIteration 244, loss = 0.45530409\nIteration 245, loss = 0.45349486\nIteration 246, loss = 0.45168415\nIteration 247, loss = 0.44987212\nIteration 248, loss = 0.44805902\nIteration 249, loss = 0.44624512\nIteration 250, loss = 0.44443073\nIteration 251, loss = 0.44261619\nIteration 252, loss = 0.44080185\nIteration 253, loss = 0.43898811\nIteration 254, loss = 0.43717536\nIteration 255, loss = 0.43536402\nIteration 256, loss = 0.43355452\nIteration 257, loss = 0.43174727\nIteration 258, loss = 0.42994271\nIteration 259, loss = 0.42814126\nIteration 260, loss = 0.42634336\nIteration 261, loss = 0.42454943\nIteration 262, loss = 0.42275987\nIteration 263, loss = 0.42097509\nIteration 264, loss = 0.41919549\nIteration 265, loss = 0.41742145\nIteration 266, loss = 0.41565335\nIteration 267, loss = 0.41389154\nIteration 268, loss = 0.41213638\nIteration 269, loss = 0.41038819\nIteration 270, loss = 0.40864731\nIteration 271, loss = 0.40691402\nIteration 272, loss = 0.40518865\nIteration 273, loss = 0.40347145\nIteration 274, loss = 0.40176271\nIteration 275, loss = 0.40006268\nIteration 276, loss = 0.39837160\nIteration 277, loss = 0.39668972\nIteration 278, loss = 0.39501724\nIteration 279, loss = 0.39335439\nIteration 280, loss = 0.39170136\nIteration 281, loss = 0.39005835\nIteration 282, loss = 0.38842552\nIteration 283, loss = 0.38680306\nIteration 284, loss = 0.38519112\nIteration 285, loss = 0.38358985\nIteration 286, loss = 0.38199940\nIteration 287, loss = 0.38041989\nIteration 288, loss = 0.37885145\nIteration 289, loss = 0.37729420\nIteration 290, loss = 0.37574823\nIteration 291, loss = 0.37421366\nIteration 292, loss = 0.37269056\nIteration 293, loss = 0.37117902\nIteration 294, loss = 0.36967912\nIteration 295, loss = 0.36819091\nIteration 296, loss = 0.36671446\nIteration 297, loss = 0.36524982\nIteration 298, loss = 0.36379703\nIteration 299, loss = 0.36235612\nIteration 300, loss = 0.36092713\nIteration 301, loss = 0.35951008\nIteration 302, loss = 0.35810499\nIteration 303, loss = 0.35671185\nIteration 304, loss = 0.35533069\nIteration 305, loss = 0.35396148\nIteration 306, loss = 0.35260423\nIteration 307, loss = 0.35125892\nIteration 308, loss = 0.34992553\nIteration 309, loss = 0.34860403\nIteration 310, loss = 0.34729441\nIteration 311, loss = 0.34599661\nIteration 312, loss = 0.34471061\nIteration 313, loss = 0.34343635\nIteration 314, loss = 0.34217380\nIteration 315, loss = 0.34092291\nIteration 316, loss = 0.33968361\nIteration 317, loss = 0.33845585\nIteration 318, loss = 0.33723958\nIteration 319, loss = 0.33603472\nIteration 320, loss = 0.33484121\nIteration 321, loss = 0.33365899\nIteration 322, loss = 0.33248798\nIteration 323, loss = 0.33132811\nIteration 324, loss = 0.33017931\nIteration 325, loss = 0.32904149\nIteration 326, loss = 0.32791458\nIteration 327, loss = 0.32679850\nIteration 328, loss = 0.32569317\nIteration 329, loss = 0.32459851\nIteration 330, loss = 0.32351443\nIteration 331, loss = 0.32244084\nIteration 332, loss = 0.32137767\nIteration 333, loss = 0.32032482\nIteration 334, loss = 0.31928221\nIteration 335, loss = 0.31824975\nIteration 336, loss = 0.31722736\nIteration 337, loss = 0.31621494\nIteration 338, loss = 0.31521241\nIteration 339, loss = 0.31421968\nIteration 340, loss = 0.31323665\nIteration 341, loss = 0.31226325\nIteration 342, loss = 0.31129938\nIteration 343, loss = 0.31034495\nIteration 344, loss = 0.30939988\nIteration 345, loss = 0.30846407\nIteration 346, loss = 0.30753744\nIteration 347, loss = 0.30661989\nIteration 348, loss = 0.30571135\nIteration 349, loss = 0.30481172\nIteration 350, loss = 0.30392092\nIteration 351, loss = 0.30303885\nIteration 352, loss = 0.30216544\nIteration 353, loss = 0.30130060\nIteration 354, loss = 0.30044423\nIteration 355, loss = 0.29959627\nIteration 356, loss = 0.29875661\nIteration 357, loss = 0.29792519\nIteration 358, loss = 0.29710190\nIteration 359, loss = 0.29628668\nIteration 360, loss = 0.29547944\nIteration 361, loss = 0.29468010\nIteration 362, loss = 0.29388857\nIteration 363, loss = 0.29310478\nIteration 364, loss = 0.29232865\nIteration 365, loss = 0.29156010\nIteration 366, loss = 0.29079904\nIteration 367, loss = 0.29004540\nIteration 368, loss = 0.28929911\nIteration 369, loss = 0.28856008\nIteration 370, loss = 0.28782825\nIteration 371, loss = 0.28710353\nIteration 372, loss = 0.28638585\nIteration 373, loss = 0.28567514\nIteration 374, loss = 0.28497133\nIteration 375, loss = 0.28427433\nIteration 376, loss = 0.28358409\nIteration 377, loss = 0.28290052\nIteration 378, loss = 0.28222356\nIteration 379, loss = 0.28155314\nIteration 380, loss = 0.28088918\nIteration 381, loss = 0.28023163\nIteration 382, loss = 0.27958041\nIteration 383, loss = 0.27893545\nIteration 384, loss = 0.27829669\nIteration 385, loss = 0.27766406\nIteration 386, loss = 0.27703750\nIteration 387, loss = 0.27641694\nIteration 388, loss = 0.27580232\nIteration 389, loss = 0.27519358\nIteration 390, loss = 0.27459064\nIteration 391, loss = 0.27399346\nIteration 392, loss = 0.27340197\nIteration 393, loss = 0.27281610\nIteration 394, loss = 0.27223580\nIteration 395, loss = 0.27166101\nIteration 396, loss = 0.27109167\nIteration 397, loss = 0.27052772\nIteration 398, loss = 0.26996911\nIteration 399, loss = 0.26941577\nIteration 400, loss = 0.26886765\nIteration 401, loss = 0.26832470\nIteration 402, loss = 0.26778685\nIteration 403, loss = 0.26725406\nIteration 404, loss = 0.26672627\nIteration 405, loss = 0.26620343\nIteration 406, loss = 0.26568548\nIteration 407, loss = 0.26517238\nIteration 408, loss = 0.26466406\nIteration 409, loss = 0.26416048\nIteration 410, loss = 0.26366160\nIteration 411, loss = 0.26316735\nIteration 412, loss = 0.26267769\nIteration 413, loss = 0.26219257\nIteration 414, loss = 0.26171194\nIteration 415, loss = 0.26123576\nIteration 416, loss = 0.26076397\nIteration 417, loss = 0.26029654\nIteration 418, loss = 0.25983340\nIteration 419, loss = 0.25937453\nIteration 420, loss = 0.25891987\nIteration 421, loss = 0.25846937\nIteration 422, loss = 0.25802301\nIteration 423, loss = 0.25758072\nIteration 424, loss = 0.25714247\nIteration 425, loss = 0.25670821\nIteration 426, loss = 0.25627790\nIteration 427, loss = 0.25585151\nIteration 428, loss = 0.25542899\nIteration 429, loss = 0.25501029\nIteration 430, loss = 0.25459538\nIteration 431, loss = 0.25418422\nIteration 432, loss = 0.25377677\nIteration 433, loss = 0.25337299\nIteration 434, loss = 0.25297284\nIteration 435, loss = 0.25257628\nIteration 436, loss = 0.25218328\nIteration 437, loss = 0.25179380\nIteration 438, loss = 0.25140780\nIteration 439, loss = 0.25102524\nIteration 440, loss = 0.25064610\nIteration 441, loss = 0.25027032\nIteration 442, loss = 0.24989789\nIteration 443, loss = 0.24952876\nIteration 444, loss = 0.24916290\nIteration 445, loss = 0.24880028\nIteration 446, loss = 0.24844085\nIteration 447, loss = 0.24808460\nIteration 448, loss = 0.24773148\nIteration 449, loss = 0.24738147\nIteration 450, loss = 0.24703453\nIteration 451, loss = 0.24669063\nIteration 452, loss = 0.24634973\nIteration 453, loss = 0.24601182\nIteration 454, loss = 0.24567685\nIteration 455, loss = 0.24534480\nIteration 456, loss = 0.24501564\nIteration 457, loss = 0.24468933\nIteration 458, loss = 0.24436585\nIteration 459, loss = 0.24404517\nIteration 460, loss = 0.24372726\nIteration 461, loss = 0.24341210\nIteration 462, loss = 0.24309965\nIteration 463, loss = 0.24278988\nIteration 464, loss = 0.24248278\nIteration 465, loss = 0.24217831\nIteration 466, loss = 0.24187644\nIteration 467, loss = 0.24157715\nIteration 468, loss = 0.24128042\nIteration 469, loss = 0.24098622\nIteration 470, loss = 0.24069451\nIteration 471, loss = 0.24040529\nIteration 472, loss = 0.24011851\nIteration 473, loss = 0.23983417\nIteration 474, loss = 0.23955222\nIteration 475, loss = 0.23927266\nIteration 476, loss = 0.23899545\nIteration 477, loss = 0.23872057\nIteration 478, loss = 0.23844800\nIteration 479, loss = 0.23817772\nIteration 480, loss = 0.23790970\nIteration 481, loss = 0.23764391\nIteration 482, loss = 0.23738035\nIteration 483, loss = 0.23711898\nIteration 484, loss = 0.23685978\nIteration 485, loss = 0.23660274\nIteration 486, loss = 0.23634783\nIteration 487, loss = 0.23609503\nIteration 488, loss = 0.23584431\nIteration 489, loss = 0.23559567\nIteration 490, loss = 0.23534907\nIteration 491, loss = 0.23510450\nIteration 492, loss = 0.23486194\nIteration 493, loss = 0.23462136\nIteration 494, loss = 0.23438276\nIteration 495, loss = 0.23414610\nIteration 496, loss = 0.23391138\nIteration 497, loss = 0.23367856\nIteration 498, loss = 0.23344764\nIteration 499, loss = 0.23321859\nIteration 500, loss = 0.23299140\nIteration 501, loss = 0.23276605\nIteration 502, loss = 0.23254251\nIteration 503, loss = 0.23232078\nIteration 504, loss = 0.23210083\nIteration 505, loss = 0.23188265\nIteration 506, loss = 0.23166622\nIteration 507, loss = 0.23145152\nIteration 508, loss = 0.23123854\nIteration 509, loss = 0.23102726\nIteration 510, loss = 0.23081767\nIteration 511, loss = 0.23060974\nIteration 512, loss = 0.23040346\nIteration 513, loss = 0.23019882\nIteration 514, loss = 0.22999579\nIteration 515, loss = 0.22979437\nIteration 516, loss = 0.22959454\nIteration 517, loss = 0.22939629\nIteration 518, loss = 0.22919959\nIteration 519, loss = 0.22900443\nIteration 520, loss = 0.22881081\nIteration 521, loss = 0.22861870\nIteration 522, loss = 0.22842808\nIteration 523, loss = 0.22823896\nIteration 524, loss = 0.22805130\nIteration 525, loss = 0.22786510\nIteration 526, loss = 0.22768035\nIteration 527, loss = 0.22749702\nIteration 528, loss = 0.22731512\nIteration 529, loss = 0.22713461\nIteration 530, loss = 0.22695550\nIteration 531, loss = 0.22677776\nIteration 532, loss = 0.22660138\nIteration 533, loss = 0.22642636\nIteration 534, loss = 0.22625267\nIteration 535, loss = 0.22608031\nIteration 536, loss = 0.22590926\nIteration 537, loss = 0.22573951\nIteration 538, loss = 0.22557105\nIteration 539, loss = 0.22540387\nIteration 540, loss = 0.22523795\nIteration 541, loss = 0.22507329\nIteration 542, loss = 0.22490986\nIteration 543, loss = 0.22474767\nIteration 544, loss = 0.22458669\nIteration 545, loss = 0.22442692\nIteration 546, loss = 0.22426834\nIteration 547, loss = 0.22411095\nIteration 548, loss = 0.22395473\nIteration 549, loss = 0.22379967\nIteration 550, loss = 0.22364577\nIteration 551, loss = 0.22349300\nIteration 552, loss = 0.22334137\nIteration 553, loss = 0.22319086\nIteration 554, loss = 0.22304145\nIteration 555, loss = 0.22289315\nIteration 556, loss = 0.22274593\nIteration 557, loss = 0.22259979\nIteration 558, loss = 0.22245473\nIteration 559, loss = 0.22231072\nIteration 560, loss = 0.22216776\nIteration 561, loss = 0.22202584\nIteration 562, loss = 0.22188495\nIteration 563, loss = 0.22174509\nIteration 564, loss = 0.22160623\nIteration 565, loss = 0.22146838\nIteration 566, loss = 0.22133152\nIteration 567, loss = 0.22119565\nIteration 568, loss = 0.22106075\nIteration 569, loss = 0.22092682\nIteration 570, loss = 0.22079384\nIteration 571, loss = 0.22066182\nIteration 572, loss = 0.22053073\nIteration 573, loss = 0.22040058\nIteration 574, loss = 0.22027135\nIteration 575, loss = 0.22014304\nIteration 576, loss = 0.22001563\nIteration 577, loss = 0.21988912\nIteration 578, loss = 0.21976350\nIteration 579, loss = 0.21963877\nIteration 580, loss = 0.21951491\nIteration 581, loss = 0.21939191\nIteration 582, loss = 0.21926978\nIteration 583, loss = 0.21914850\nIteration 584, loss = 0.21902806\nIteration 585, loss = 0.21890845\nIteration 586, loss = 0.21878968\nIteration 587, loss = 0.21867173\nIteration 588, loss = 0.21855459\nIteration 589, loss = 0.21843826\nIteration 590, loss = 0.21832272\nIteration 591, loss = 0.21820798\nIteration 592, loss = 0.21809403\nIteration 593, loss = 0.21798085\nIteration 594, loss = 0.21786845\nIteration 595, loss = 0.21775681\nIteration 596, loss = 0.21764593\nIteration 597, loss = 0.21753580\nIteration 598, loss = 0.21742642\nIteration 599, loss = 0.21731777\nIteration 600, loss = 0.21720985\nIteration 601, loss = 0.21710266\nIteration 602, loss = 0.21699619\nIteration 603, loss = 0.21689043\nIteration 604, loss = 0.21678538\nIteration 605, loss = 0.21668103\nIteration 606, loss = 0.21657737\nIteration 607, loss = 0.21647440\nIteration 608, loss = 0.21637211\nIteration 609, loss = 0.21627050\nIteration 610, loss = 0.21616956\nIteration 611, loss = 0.21606928\nIteration 612, loss = 0.21596966\nIteration 613, loss = 0.21587070\nIteration 614, loss = 0.21577238\nIteration 615, loss = 0.21567470\nIteration 616, loss = 0.21557766\nIteration 617, loss = 0.21548125\nIteration 618, loss = 0.21538547\nIteration 619, loss = 0.21529030\nIteration 620, loss = 0.21519575\nIteration 621, loss = 0.21510181\nIteration 622, loss = 0.21500847\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
    },
    {
     "data": {
      "text/plain": "MLPClassifier(activation='logistic', alpha=0.1, batch_size='auto', beta_1=0.9,\n              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=(1, 3), learning_rate='constant',\n              learning_rate_init=0.1, max_fun=15000, max_iter=1000,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=1, shuffle=True, solver='sgd',\n              tol=0.0001, validation_fraction=0.1, verbose=True,\n              warm_start=False)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = skMLP(solver='sgd', alpha=0.1, learning_rate_init=0.1, hidden_layer_sizes=(1,3), activation='logistic', max_iter=1000, random_state=1, verbose=True)\n",
    "\n",
    "mlp.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training set total iterations: 622\n           Training set score: 96.00%\n            Training set loss: 0.21501\n             Number of output: 3\n              Number of layer: 4\n\nLoss:\nLayer: 0\n [2.95321137]\nLayer: 1\n [ 1.71625189  3.99231484 -1.563244  ]\nLayer: 2\n [ 2.39546079  0.71942836 -2.44235655]\n\nWeight:\nLayer: 0\n [[ 0.50803757]\n [ 1.89315219]\n [-3.23071367]\n [-4.24337168]]\nLayer: 1\n [[-5.8821578  -5.74953572  4.67013753]]\nLayer: 2\n [[-3.58808463 -1.68534776  5.47831888]\n [-6.80100172  2.06963982  4.43691932]\n [ 3.75845381  0.7633835  -4.98305845]]\n\nTest accuracy: 96.00%\n"
    }
   ],
   "source": [
    "print('Training set total iterations:', mlp.n_iter_)\n",
    "print('           Training set score: ' + '{:.2%}'.format(mlp.score(trainX, trainY)))\n",
    "print('            Training set loss: %.5f' % mlp.loss_)\n",
    "print('             Number of output:', mlp.n_outputs_)\n",
    "print('              Number of layer:', mlp.n_layers_)\n",
    "\n",
    "print('\\nLoss:')\n",
    "print('Layer: 0\\n', mlp.intercepts_[0])\n",
    "print('Layer: 1\\n', mlp.intercepts_[1])\n",
    "print('Layer: 2\\n', mlp.intercepts_[2])\n",
    "\n",
    "print('\\nWeight:')\n",
    "print('Layer: 0\\n', mlp.coefs_[0])\n",
    "print('Layer: 1\\n', mlp.coefs_[1])\n",
    "print('Layer: 2\\n', mlp.coefs_[2])\n",
    "\n",
    "prediction = mlp.predict(testX)\n",
    "print('\\nTest accuracy: ' + '{:.2%}'.format(metrics.accuracy_score(prediction,testY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Pembagian Tugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| NIM      | Nama              | Tugas                                         |\n",
    "|:--------:|:------------------|:----------------------------------------------|\n",
    "| 13517014 | Yoel Susanto      | Feed Forward, Backpropagation, Data Structure |\n",
    "| 13517065 | Andrian Cedric    | Feed Forward, Function, Documentation         |\n",
    "| 13517131 | Jan Meyer Saragih | Feed Forward, Backpropagation, Data Structure |\n",
    "| 13517137 | Vincent Budianto  | Feed Forward, Function, Documentation         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}