{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets as dataset\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.tree as tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris = dataset.load_iris()\n",
    "# tennis = pd.read_csv('tennis.csv')\n",
    "# tHead = list(tennis.columns)\n",
    "# tennis = np.array(tennis)\n",
    "mlp = MLP(solver='sgd', alpha=0.1, learning_rate_init=0.1, hidden_layer_sizes=(1,3), activation='logistic', max_iter=1000, random_state=1, verbose=10)\n",
    "encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12264498\n",
      "Iteration 2, loss = 1.11796123\n",
      "Iteration 3, loss = 1.11238035\n",
      "Iteration 4, loss = 1.10687986\n",
      "Iteration 5, loss = 1.10223042\n",
      "Iteration 6, loss = 1.09889148\n",
      "Iteration 7, loss = 1.09698044\n",
      "Iteration 8, loss = 1.09631527\n",
      "Iteration 9, loss = 1.09651358\n",
      "Iteration 10, loss = 1.09711710\n",
      "Iteration 11, loss = 1.09770689\n",
      "Iteration 12, loss = 1.09798291\n",
      "Iteration 13, loss = 1.09779624\n",
      "Iteration 14, loss = 1.09713818\n",
      "Iteration 15, loss = 1.09610017\n",
      "Iteration 16, loss = 1.09482257\n",
      "Iteration 17, loss = 1.09344768\n",
      "Iteration 18, loss = 1.09208713\n",
      "Iteration 19, loss = 1.09080676\n",
      "Iteration 20, loss = 1.08962721\n",
      "Iteration 21, loss = 1.08853499\n",
      "Iteration 22, loss = 1.08749796\n",
      "Iteration 23, loss = 1.08647989\n",
      "Iteration 24, loss = 1.08545066\n",
      "Iteration 25, loss = 1.08439066\n",
      "Iteration 26, loss = 1.08329003\n",
      "Iteration 27, loss = 1.08214468\n",
      "Iteration 28, loss = 1.08095158\n",
      "Iteration 29, loss = 1.07970508\n",
      "Iteration 30, loss = 1.07839542\n",
      "Iteration 31, loss = 1.07700904\n",
      "Iteration 32, loss = 1.07553026\n",
      "Iteration 33, loss = 1.07394330\n",
      "Iteration 34, loss = 1.07223417\n",
      "Iteration 35, loss = 1.07039177\n",
      "Iteration 36, loss = 1.06840807\n",
      "Iteration 37, loss = 1.06627733\n",
      "Iteration 38, loss = 1.06399477\n",
      "Iteration 39, loss = 1.06155516\n",
      "Iteration 40, loss = 1.05895181\n",
      "Iteration 41, loss = 1.05617613\n",
      "Iteration 42, loss = 1.05321787\n",
      "Iteration 43, loss = 1.05006558\n",
      "Iteration 44, loss = 1.04670729\n",
      "Iteration 45, loss = 1.04313102\n",
      "Iteration 46, loss = 1.03932514\n",
      "Iteration 47, loss = 1.03527831\n",
      "Iteration 48, loss = 1.03097940\n",
      "Iteration 49, loss = 1.02641720\n",
      "Iteration 50, loss = 1.02158026\n",
      "Iteration 51, loss = 1.01645694\n",
      "Iteration 52, loss = 1.01103575\n",
      "Iteration 53, loss = 1.00530586\n",
      "Iteration 54, loss = 0.99925764\n",
      "Iteration 55, loss = 0.99288311\n",
      "Iteration 56, loss = 0.98617608\n",
      "Iteration 57, loss = 0.97913209\n",
      "Iteration 58, loss = 0.97174827\n",
      "Iteration 59, loss = 0.96402310\n",
      "Iteration 60, loss = 0.95595648\n",
      "Iteration 61, loss = 0.94754981\n",
      "Iteration 62, loss = 0.93880640\n",
      "Iteration 63, loss = 0.92973177\n",
      "Iteration 64, loss = 0.92033405\n",
      "Iteration 65, loss = 0.91062423\n",
      "Iteration 66, loss = 0.90061629\n",
      "Iteration 67, loss = 0.89032731\n",
      "Iteration 68, loss = 0.87977746\n",
      "Iteration 69, loss = 0.86898997\n",
      "Iteration 70, loss = 0.85799105\n",
      "Iteration 71, loss = 0.84680981\n",
      "Iteration 72, loss = 0.83547804\n",
      "Iteration 73, loss = 0.82402996\n",
      "Iteration 74, loss = 0.81250193\n",
      "Iteration 75, loss = 0.80093195\n",
      "Iteration 76, loss = 0.78935924\n",
      "Iteration 77, loss = 0.77782363\n",
      "Iteration 78, loss = 0.76636499\n",
      "Iteration 79, loss = 0.75502262\n",
      "Iteration 80, loss = 0.74383460\n",
      "Iteration 81, loss = 0.73283724\n",
      "Iteration 82, loss = 0.72206447\n",
      "Iteration 83, loss = 0.71154738\n",
      "Iteration 84, loss = 0.70131383\n",
      "Iteration 85, loss = 0.69138809\n",
      "Iteration 86, loss = 0.68179065\n",
      "Iteration 87, loss = 0.67253814\n",
      "Iteration 88, loss = 0.66364328\n",
      "Iteration 89, loss = 0.65511497\n",
      "Iteration 90, loss = 0.64695849\n",
      "Iteration 91, loss = 0.63917571\n",
      "Iteration 92, loss = 0.63176539\n",
      "Iteration 93, loss = 0.62472349\n",
      "Iteration 94, loss = 0.61804352\n",
      "Iteration 95, loss = 0.61171693\n",
      "Iteration 96, loss = 0.60573341\n",
      "Iteration 97, loss = 0.60008125\n",
      "Iteration 98, loss = 0.59474768\n",
      "Iteration 99, loss = 0.58971913\n",
      "Iteration 100, loss = 0.58498151\n",
      "Iteration 101, loss = 0.58052042\n",
      "Iteration 102, loss = 0.57632140\n",
      "Iteration 103, loss = 0.57237001\n",
      "Iteration 104, loss = 0.56865206\n",
      "Iteration 105, loss = 0.56515365\n",
      "Iteration 106, loss = 0.56186131\n",
      "Iteration 107, loss = 0.55876203\n",
      "Iteration 108, loss = 0.55584332\n",
      "Iteration 109, loss = 0.55309325\n",
      "Iteration 110, loss = 0.55050044\n",
      "Iteration 111, loss = 0.54805411\n",
      "Iteration 112, loss = 0.54574404\n",
      "Iteration 113, loss = 0.54356059\n",
      "Iteration 114, loss = 0.54149466\n",
      "Iteration 115, loss = 0.53953769\n",
      "Iteration 116, loss = 0.53768162\n",
      "Iteration 117, loss = 0.53591888\n",
      "Iteration 118, loss = 0.53424239\n",
      "Iteration 119, loss = 0.53264546\n",
      "Iteration 120, loss = 0.53112185\n",
      "Iteration 121, loss = 0.52966567\n",
      "Iteration 122, loss = 0.52827143\n",
      "Iteration 123, loss = 0.52693394\n",
      "Iteration 124, loss = 0.52564832\n",
      "Iteration 125, loss = 0.52441001\n",
      "Iteration 126, loss = 0.52321466\n",
      "Iteration 127, loss = 0.52205822\n",
      "Iteration 128, loss = 0.52093681\n",
      "Iteration 129, loss = 0.51984678\n",
      "Iteration 130, loss = 0.51878465\n",
      "Iteration 131, loss = 0.51774711\n",
      "Iteration 132, loss = 0.51673100\n",
      "Iteration 133, loss = 0.51573329\n",
      "Iteration 134, loss = 0.51475109\n",
      "Iteration 135, loss = 0.51378158\n",
      "Iteration 136, loss = 0.51282210\n",
      "Iteration 137, loss = 0.51187004\n",
      "Iteration 138, loss = 0.51092287\n",
      "Iteration 139, loss = 0.50997817\n",
      "Iteration 140, loss = 0.50903354\n",
      "Iteration 141, loss = 0.50808666\n",
      "Iteration 142, loss = 0.50713526\n",
      "Iteration 143, loss = 0.50617711\n",
      "Iteration 144, loss = 0.50521000\n",
      "Iteration 145, loss = 0.50423178\n",
      "Iteration 146, loss = 0.50324033\n",
      "Iteration 147, loss = 0.50223354\n",
      "Iteration 148, loss = 0.50120934\n",
      "Iteration 149, loss = 0.50016572\n",
      "Iteration 150, loss = 0.49910066\n",
      "Iteration 151, loss = 0.49801222\n",
      "Iteration 152, loss = 0.49689848\n",
      "Iteration 153, loss = 0.49575758\n",
      "Iteration 154, loss = 0.49458773\n",
      "Iteration 155, loss = 0.49338720\n",
      "Iteration 156, loss = 0.49215436\n",
      "Iteration 157, loss = 0.49088764\n",
      "Iteration 158, loss = 0.48958559\n",
      "Iteration 159, loss = 0.48824690\n",
      "Iteration 160, loss = 0.48687036\n",
      "Iteration 161, loss = 0.48545493\n",
      "Iteration 162, loss = 0.48399972\n",
      "Iteration 163, loss = 0.48250403\n",
      "Iteration 164, loss = 0.48096732\n",
      "Iteration 165, loss = 0.47938929\n",
      "Iteration 166, loss = 0.47776980\n",
      "Iteration 167, loss = 0.47610897\n",
      "Iteration 168, loss = 0.47440711\n",
      "Iteration 169, loss = 0.47266476\n",
      "Iteration 170, loss = 0.47088268\n",
      "Iteration 171, loss = 0.46906186\n",
      "Iteration 172, loss = 0.46720345\n",
      "Iteration 173, loss = 0.46530886\n",
      "Iteration 174, loss = 0.46337962\n",
      "Iteration 175, loss = 0.46141745\n",
      "Iteration 176, loss = 0.45942423\n",
      "Iteration 177, loss = 0.45740193\n",
      "Iteration 178, loss = 0.45535262\n",
      "Iteration 179, loss = 0.45327847\n",
      "Iteration 180, loss = 0.45118166\n",
      "Iteration 181, loss = 0.44906443\n",
      "Iteration 182, loss = 0.44692900\n",
      "Iteration 183, loss = 0.44477757\n",
      "Iteration 184, loss = 0.44261230\n",
      "Iteration 185, loss = 0.44043529\n",
      "Iteration 186, loss = 0.43824857\n",
      "Iteration 187, loss = 0.43605409\n",
      "Iteration 188, loss = 0.43385369\n",
      "Iteration 189, loss = 0.43164915\n",
      "Iteration 190, loss = 0.42944211\n",
      "Iteration 191, loss = 0.42723413\n",
      "Iteration 192, loss = 0.42502667\n",
      "Iteration 193, loss = 0.42282111\n",
      "Iteration 194, loss = 0.42061873\n",
      "Iteration 195, loss = 0.41842073\n",
      "Iteration 196, loss = 0.41622825\n",
      "Iteration 197, loss = 0.41404234\n",
      "Iteration 198, loss = 0.41186401\n",
      "Iteration 199, loss = 0.40969423\n",
      "Iteration 200, loss = 0.40753387\n",
      "Iteration 201, loss = 0.40538381\n",
      "Iteration 202, loss = 0.40324486\n",
      "Iteration 203, loss = 0.40111779\n",
      "Iteration 204, loss = 0.39900332\n",
      "Iteration 205, loss = 0.39690217\n",
      "Iteration 206, loss = 0.39481497\n",
      "Iteration 207, loss = 0.39274236\n",
      "Iteration 208, loss = 0.39068490\n",
      "Iteration 209, loss = 0.38864313\n",
      "Iteration 210, loss = 0.38661755\n",
      "Iteration 211, loss = 0.38460860\n",
      "Iteration 212, loss = 0.38261671\n",
      "Iteration 213, loss = 0.38064223\n",
      "Iteration 214, loss = 0.37868549\n",
      "Iteration 215, loss = 0.37674679\n",
      "Iteration 216, loss = 0.37482635\n",
      "Iteration 217, loss = 0.37292438\n",
      "Iteration 218, loss = 0.37104106\n",
      "Iteration 219, loss = 0.36917651\n",
      "Iteration 220, loss = 0.36733082\n",
      "Iteration 221, loss = 0.36550405\n",
      "Iteration 222, loss = 0.36369625\n",
      "Iteration 223, loss = 0.36190740\n",
      "Iteration 224, loss = 0.36013750\n",
      "Iteration 225, loss = 0.35838648\n",
      "Iteration 226, loss = 0.35665428\n",
      "Iteration 227, loss = 0.35494081\n",
      "Iteration 228, loss = 0.35324597\n",
      "Iteration 229, loss = 0.35156962\n",
      "Iteration 230, loss = 0.34991163\n",
      "Iteration 231, loss = 0.34827185\n",
      "Iteration 232, loss = 0.34665012\n",
      "Iteration 233, loss = 0.34504626\n",
      "Iteration 234, loss = 0.34346010\n",
      "Iteration 235, loss = 0.34189144\n",
      "Iteration 236, loss = 0.34034010\n",
      "Iteration 237, loss = 0.33880588\n",
      "Iteration 238, loss = 0.33728858\n",
      "Iteration 239, loss = 0.33578800\n",
      "Iteration 240, loss = 0.33430393\n",
      "Iteration 241, loss = 0.33283617\n",
      "Iteration 242, loss = 0.33138452\n",
      "Iteration 243, loss = 0.32994883\n",
      "Iteration 244, loss = 0.32852930\n",
      "Iteration 245, loss = 0.32712883\n",
      "Iteration 246, loss = 0.32577303\n",
      "Iteration 247, loss = 0.32467766\n",
      "Iteration 248, loss = 0.32565539\n",
      "Iteration 249, loss = 0.34436659\n",
      "Iteration 250, loss = 0.46530694\n",
      "Iteration 251, loss = 0.75760556\n",
      "Iteration 252, loss = 0.74973078\n",
      "Iteration 253, loss = 0.70831214\n",
      "Iteration 254, loss = 0.64416626\n",
      "Iteration 255, loss = 0.58347441\n",
      "Iteration 256, loss = 0.54761475\n",
      "Iteration 257, loss = 0.53408580\n",
      "Iteration 258, loss = 0.53022660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "iris_tree1 = mlp.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.666667\n"
     ]
    }
   ],
   "source": [
    "# ri1 = export_text(iris_tree1, feature_names=iris['feature_names'])\n",
    "# print(ri1)\n",
    "mlp.fit(iris.data, iris.target)\n",
    "print(\"Training set score: %f\" % mlp.score(iris.data, iris.target))\n",
    "# print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmin    : -1.7737484060305424\n",
      "vmax    : 1.274748784943507\n",
      "\n",
      "weight1 : [[-3.91539439 -4.66060092  3.06612925]]\n",
      "\n",
      "weight2 : [[-2.71915956 -0.19606554  3.17675041]\n",
      " [-5.14355379  1.68975204  3.07796347]\n",
      " [ 2.52834888  0.14020045 -3.25727044]]\n",
      "\n",
      "bias1:  [ 0.73413248  2.51538125 -0.49814509]\n",
      "\n",
      "bias2:  [ 1.7664611   0.82922324 -1.92315173]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,1)\n",
    "vmin = mlp.coefs_[0].min()\n",
    "vmax = mlp.coefs_[0].max()\n",
    "\n",
    "print('vmin    :', vmin)\n",
    "print('vmax    :', vmax)\n",
    "print()\n",
    "print('weight1 :', mlp.coefs_[1])\n",
    "print()\n",
    "print('weight2 :', mlp.coefs_[2])\n",
    "print()\n",
    "print('bias1: ', mlp.intercepts_[1])\n",
    "print()\n",
    "print('bias2: ', mlp.intercepts_[2])\n",
    "\n",
    "for coef in (mlp.coefs_[0]):\n",
    "    axes.matshow(mlp.coefs_[0], vmin=vmin, vmax=vmax)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
